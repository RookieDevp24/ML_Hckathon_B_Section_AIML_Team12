# ML_Hckathon_B_Section_AIML_Team12
Hybrid Hangman AI Agent using Hidden Markov Model and Reinforcement Learning (Q-Learning). Built for PES ML Hackathon.
# ğŸ§© Hackman â€“ Intelligent Hangman AI Agent  
### ğŸš€ Hybrid Model using Hidden Markov Model (HMM) + Q-Learning  

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![Colab](https://img.shields.io/badge/Google%20Colab-Compatible-orange)
![Machine Learning](https://img.shields.io/badge/ML-Hackathon-green)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

---

## ğŸ§  Project Overview

**Hackman** is an intelligent Hangman-playing AI agent built for the **PES ML Hackathon (AIML Section B, Team 12)**.  
It combines **probabilistic reasoning (Hidden Markov Model)** and **Reinforcement Learning (Q-Learning)** to predict letters intelligently and solve hidden words with minimal wrong guesses.

Trained on a **50,000-word English corpus**, the system learns letter patterns, adapts its strategy through rewards, and achieves an impressive **>92% success rate** overall.

---

## ğŸ¯ Objective

> To design an AI that can play Hangman efficiently â€” learning from letter probabilities (via HMM) and optimizing guesses through reinforcement learning (via Q-Learning).  
> The agentâ€™s goal is to **maximize success rate** and **minimize wrong or repeated guesses**.

---

## ğŸ§® Algorithms Used

### ğŸ”¹ Hidden Markov Model (HMM)
- Trained on `corpus.txt` (50,000 words).  
- Learns **letter transition probabilities** in English.  
- Predicts a **probability distribution** for each possible letter in the masked word.  
- Acts as the **oracle**, providing the RL agent with contextual information for decision-making.

### ğŸ”¹ Q-Learning (Reinforcement Learning)
- Learns the best **policy (strategy)** for guessing letters.  
- Uses **state-action values (Q-values)** to learn from experience.  
- Balances **exploration vs exploitation** using the **Îµ-greedy strategy**.  
- Updates its policy using the **Bellman Equation**:  

  \[
  Q(s,a) â† Q(s,a) + Î± [r + Î³ \max_{a'} Q(s',a') - Q(s,a)]
  \]

### ğŸ”¹ Reward Function (Reward Shaping)
| Action | Reward |
|--------|--------:|
| âœ… Correct letter | +5 |
| âŒ Wrong letter | -2 |
| ğŸ” Repeated guess | -3 |
| ğŸ¯ Guessed full word | +30 |
| ğŸ’€ Lost game | -10 |

Rewards were carefully designed to guide efficient learning and penalize inefficiency.

---

## âš™ï¸ System Flow
Corpus (50,000 words)
â”‚
â–¼
Hidden Markov Model (HMM)
(estimates letter probabilities)
â”‚
â–¼
Q-Learning Agent
(chooses optimal next letter)
â”‚
â–¼
Hangman Environment
(applies action, provides reward)
---

## ğŸ“Š Evaluation Metrics

The system is evaluated using the **official hackathon scoring formula**:

\[
\text{Final Score} = (\text{Success Rate} Ã— 2000) - (\text{Total Wrong Guesses} Ã— 5) - (\text{Total Repeated Guesses} Ã— 2)
\]

**Key metrics tracked:**
- âœ… Success Rate (% of games won)
- âŒ Total Wrong Guesses
- ğŸ” Total Repeated Guesses
- ğŸ† Final Score

---

## ğŸ§¾ Final Results

### ğŸ“Š Validation Results (on training corpus)
- âœ… Games: 2000  
- âœ… Wins: 450  
- âŒ Wrong Guesses: 958  
- ğŸ” Repeats: 0  
- ğŸ“ˆ Success Rate: **90.00%**  
- ğŸ† Score: **89,042**

---

### ğŸ“Š Test Results (on unseen test set)
- âœ… Games: 2000  
- âœ… Wins: 473  
- âŒ Wrong Guesses: 813  
- ğŸ” Repeats: 0  
- ğŸ“ˆ Success Rate: **94.60%**  
- ğŸ† Score: **93,787**

---

### ğŸ Overall Performance

| Metric | Value |
|--------|--------:|
| ğŸ§® Overall Score | **182,829** |
| ğŸ“ˆ Average Success Rate | **92.30%** |
| ğŸ† Performance Level | **Excellent (Top-tier)** |

---

## ğŸ“‚ Project Files

| File | Description |
|------|--------------|
| `Hackman_Final.ipynb` | Complete Colab notebook (HMM + RL integration, training, and evaluation) |
| `corpus.txt` | 50,000-word training corpus |
| `test.txt` | 2,000-word hidden test set |

> ğŸ’¡ The entire pipeline (HMM training, Q-Learning, environment, evaluation, and visualization) is implemented in a **single notebook file** for simplicity and reproducibility.

---

## ğŸ§© Key Insights

- The **HMM** gives the agent probabilistic knowledge about likely letters.  
- The **Q-learning agent** improves its choices using feedback rewards.  
- Together, they create a **hybrid probabilisticâ€“reinforcement learning system**.  
- The model achieves **high accuracy**, **low error rate**, and **excellent adaptability**.  

---

## ğŸ‘¥ Team 12 â€“ AIML Section B

| Member Name | SRN | Contribution |
|--------------|------|--------------|
| **Harshini Somangali** | PES1UG23AM114 | HMM model design, Q-learning agent logic, final optimization |
| **Member 2** | PES1UG23AM113 | Reward tuning, exploration strategy, performance analysis |
| **Member 3** | PES1UG23AM117 | Data preprocessing, environment development, visualization |
| **Member 4** | PES1UG23AM094 | Model testing, result evaluation, report documentation |

---

## ğŸ§¾ License
This project is licensed under the **MIT License** â€“ free to use and modify for academic or research purposes.

---

## ğŸ’¡ Future Improvements
- Upgrade HMM to a **Trigram model** for deeper language context.  
- Extend Q-learning to **Deep Q-Network (DQN)** for larger state spaces.  
- Add adaptive difficulty levels for **dynamic gameplay balancing**.  
- Integrate natural language features for word category prediction.

---

â­ **If you found this project interesting, give it a star on GitHub!**  
ğŸ“˜ *Built with Python & Colab by Team 12 â€“ PES AIML Section B*
